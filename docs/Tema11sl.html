<!DOCTYPE html>
<html>
<head>
  <title>Tema 11 - Selección y regularización del modelo lineal</title>

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="generator" content="pandoc" />




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">

  <base target="_blank">

  <script type="text/javascript">
    var SLIDE_CONFIG = {
      // Slide settings
      settings: {
                title: 'Tema 11 - Selección y regularización del modelo lineal',
                        useBuilds: true,
        usePrettify: true,
        enableSlideAreas: true,
        enableTouch: true,
                        favIcon: 'Tema11sl_files/logo.svg',
              },

      // Author information
      presenters: [
            {
        name:  'Prof.: Pedro Albarrán' ,
        company: '',
        gplus: '',
        twitter: '',
        www: '',
        github: ''
      },
            {
        name:  'Prof.: Alberto Pérez' ,
        company: '',
        gplus: '',
        twitter: '',
        www: '',
        github: ''
      },
            ]
    };
  </script>

  <link href="site_libs/ioslides-13.5.1/fonts/fonts.css" rel="stylesheet" />
  <link href="site_libs/ioslides-13.5.1/theme/css/default.css" rel="stylesheet" />
  <link href="site_libs/ioslides-13.5.1/theme/css/phone.css" rel="stylesheet" />
  <script src="site_libs/ioslides-13.5.1/js/modernizr.custom.45394.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/prettify/prettify.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/prettify/lang-r.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/prettify/lang-yaml.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/hammer.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/slide-controller.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/slide-deck.js"></script>

  <style type="text/css">

    b, strong {
      font-weight: bold;
    }

    em {
      font-style: italic;
    }

    summary {
      display: list-item;
    }

    slides > slide {
      -webkit-transition: all 0.4s ease-in-out;
      -moz-transition: all 0.4s ease-in-out;
      -o-transition: all 0.4s ease-in-out;
      transition: all 0.4s ease-in-out;
    }

    .auto-fadein {
      -webkit-transition: opacity 0.6s ease-in;
      -webkit-transition-delay: 0.4s;
      -moz-transition: opacity 0.6s ease-in 0.4s;
      -o-transition: opacity 0.6s ease-in 0.4s;
      transition: opacity 0.6s ease-in 0.4s;
      opacity: 0;
    }
/* https://github.com/ropensci/plotly/pull/524#issuecomment-468142578 */
slide:not(.current) .plotly.html-widget{
  display: block;
}

    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
            code.sourceCode > span { display: inline-block; line-height: 1.25; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode { white-space: pre; position: relative; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    code.sourceCode { white-space: pre-wrap; }
    code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
        
    slides > slide:not(.nobackground):before {
      font-size: 12pt;
      content: "";
      position: absolute;
      bottom: 20px;
      left: 60px;
      background: url(Tema11sl_files/logo.svg) no-repeat 0 50%;
      -webkit-background-size: 30px 30px;
      -moz-background-size: 30px 30px;
      -o-background-size: 30px 30px;
      background-size: 30px 30px;
      padding-left: 40px;
      height: 30px;
      line-height: 1.9;
    }
  </style>


</head>

<body style="opacity: 0">

<slides class="layout-widescreen">

  <slide class="title-slide segue nobackground">
        <aside class="gdbar"><img src="Tema11sl_files/logo.svg"></aside>
        <!-- The content of this hgroup is replaced programmatically through the slide_config.json. -->
    <hgroup class="auto-fadein">
      <h1 data-config-title><!-- populated from slide_config.json --></h1>
      
      <p data-config-presenter><!-- populated from slide_config.json --></p>
            <p style="margin-top: 6px; margin-left: -2px;">Universidad de Alicante, Curso 2020/21</p>
          </hgroup>
  </slide>

<slide class=""><hgroup><h2>Alternativas a Mínimos Cuadrados</h2></hgroup><article  id="alternativas-a-mínimos-cuadrados">

<p>\[
\small
Y = \beta_0 + \beta_1 X_1 + \dots + \beta_k X_k + \varepsilon
\]</p>

<ul>
<li><p>¿Por qué?</p>

<ol>
<li><p><strong>Precisión</strong>: <!--incluso si la verdadera relación es lineal,-->la varianza aumenta cuando crece el número de parámetros (=regresores) relativo al de observaciones</p>

<ul>
<li>con \(\small k&gt;n\) no existe una estimación única de mínimos cuadrados</li>
</ul></li>
<li><p><strong>Interpretación</strong>: un modelo con variables irrelevantes <!-- no asociadas a la de respuesta--> es más complejo y menos interpretable</p></li>
</ol></li>
<li><p>La <strong>restricción o reducción</strong> de los coeficientes estimados puede reducir la varianza (a costa de un aumento insignificante del sesgo)</p></li>
<li><p>La <strong>selección</strong> de variables se utiliza para excluir variables irrelevantes y ajustar ese modelo reducido por mínimos cuadrados</p></li>
</ul>

</article></slide><slide class=""><hgroup><h2>Selección del Mejor Subconjunto</h2></hgroup><article  id="selección-del-mejor-subconjunto">

<ul>
<li><p>Debemos estimar \(\small 2^k\) modelos posibles con cada combinación de regresores (desde un solo regresor hasta todos a la vez)</p></li>
<li><p>Usar SCR en entrenamiento lleva a elegir el modelo con \(\small k\) parámetros</p></li>
<li><p>Procedimiento:</p>

<ol>
<li><p>Para cada \(\small p=1,\dots,k\), estimar todos los modelos con \(p\) parámetros y elegir aquel con menor error (ej., SCR): \(\small M^*_p\)</p></li>
<li><p>Elegir entre los modelos \(\small M^*_1, \dots, M^*_k\) usando validación cruzada o similar</p></li>
</ol></li>
<li><p>No validamos todos, pero estimarlos es prohibitivo para \(\small k\) moderada</p></li>
</ul>

</article></slide><slide class=""><hgroup><h2>Selección paso a paso hacia adelante</h2></hgroup><article  id="selección-paso-a-paso-hacia-adelante">

<ul>
<li><p>Procedimiento: empezando por modelo sin regresores \(\small M_0\)</p>

<ol>
<li>Para cada \(\small p=0, 1,\dots,k-1\), estimar todos los modelos que añadan UN regresor a \(\small M^f_p\)</li>
<li>Elegir como modelo \(\small M^f_{p+1}\) el que tiene menor SCR</li>
<li>Elegir entre \(\small M_0, M^f_1, \dots, M^f_k\) con validación cruzada o similar</li>
</ol></li>
</ul>

<!--
* Muchos menos modelos: $\small k-p$ por iteración, en total $\small 1+\frac{p(p+1)}{2}$
-->

<ul>
<li><p>Solo \(\small 1+\frac{p(p+1)}{2}\) modelos</p></li>
<li><p>Factible aunque \(\small k&gt;n\) pero para modelo \(M_0,\dots,M^f_{n-1}\)</p></li>
<li><p>No se garantiza encontrar el mejor subconjunto, por eliminar pronto un regresor importante</p>

<ul>
<li>ej. el mejor \(\small M^*_2\) no usa el regresor de <!--del mejor modelo de un regresor--> \(\small M^f_1=M^*_1\)</li>
</ul></li>
</ul>

</article></slide><slide class=""><hgroup><h2>Selección paso a paso hacia atrás</h2></hgroup><article  id="selección-paso-a-paso-hacia-atrás">

<ul>
<li><p>Procedimiento: empezando por modelo con todos los regresores \(\small M_k\)</p>

<ol>
<li>Para cada \(\small p=k, k-1, \dots,1\), estimar todos los modelos que eliminan UN regresor a \(\small M^b_p\)</li>
<li>Elegir como modelo \(\small M^b_{p-1}\) el que tiene menor SCR</li>
<li>Elegir entre \(\small M_0, M^b_1, \dots, M^b_k\) usando validación cruzada o similar</li>
</ol></li>
<li><p>Solo \(\small 1+\frac{p(p+1)}{2}\) modelos</p></li>
<li><p>Pero no factible si \(\small k&gt;n\) (no se puede ajustar \(M_{k}\))</p></li>
<li><p>NADA garantiza acabar con el mejor subconjunto de regresores</p></li>
</ul>

</article></slide><slide class=""><hgroup><h2>Otros procedimientos</h2></hgroup><article  id="otros-procedimientos">

<ul>
<li><p><strong>Selección mixta</strong> de subconjuntos: en cada iteración se añaden variables de forma secuencial, pero también se eliminan las que ya no mejoren el ajuste</p>

<ul>
<li>simulan la selección de mejores subconjuntos, con las ventajas computacionales de selección por pasos.</li>
</ul></li>
<li><p>Estimar <em>directamente</em> el error de prueba mediante validación cruzada</p></li>
<li><p>Estimar <em>indirectamente</em> el error de prueba mediante <strong>ajustes</strong> en el error de entrenamiento para tener en cuenta el sesgo por &ldquo;overfitting&rdquo;</p></li>
<li><p>Todos estos métodos (y los anteriores de selección) se pueden usar también en regresión logística</p></li>
</ul>

</article></slide><slide class=""><hgroup><h2>Ajustes mediante penalización</h2></hgroup><article  id="ajustes-mediante-penalización">

<ul>
<li><p>Sea \(\small n\) el número de observaciones, \(\small p\) el número de parámetros y \(\small \widehat{\sigma}^2\) un estimación de la varianza del error \(\small \varepsilon\)</p></li>
<li><p>\(\small C_p\) de Mallow: \(C_p = \frac{1}{n}\left( SCR + 2 d \widehat{\sigma}^2 \right)\)</p></li>
<li><p>Criterio de Información de Akaike \(AIC = - 2 log L + 2 d\)</p>

<ul>
<li>\(\small L\)=valor maximizado de la función de verosimilitud</li>
</ul></li>
<li><p>En modelos lineales con errores normales \(C_p = AIC\)</p></li>
<li><p>Criterio de Información Bayesiano: \(BIC = \frac{1}{n}\left( SCR + log(n) d \widehat{\sigma}^2 \right)\)</p>

<ul>
<li>\(\small n&gt;7 \Rightarrow log(n)&gt;2\), \(\small BIC\) penaliza más modelos con más parámetros</li>
</ul></li>
<li><p>\(R^2-ajustado = 1- \frac{SCR/(n-d-1)}{SCT/(n-1)}\) o \(SCR/(n-d-1)\)</p></li>
</ul>

</article></slide><slide class=""><hgroup><h2>Ajustar o Validar</h2></hgroup><article  id="ajustar-o-validar">

<ul>
<li><p>Validación cruzada ofrece una estimación <strong>más directa</strong></p></li>
<li><p>Los métodos de ajuste ofrecen una estimación indirecta a través de supuestos que pueden ser erróneos</p></li>
<li><p>Validación cruzada es computacionalmente <strong>más costosa</strong></p></li>
<li><p>Validación cruzada NO necesita estimar \(\widehat{\sigma}^2\) (puede ser difícil en algunos modelos)</p></li>
<li><p><strong>Regla de parquedad paramétrica</strong> o <strong>de un error estándar</strong>: dado un conjunto de modelos igualmente buenos, es mejor elegir el modelo más simple</p>

<ul>
<li>seleccionar el modelo con menos variables que esté dentro de un error estándar del menor error de prueba estimado.</li>
</ul></li>
</ul>

</article></slide><slide class=""><hgroup><h2>Métodos de regularización</h2></hgroup><article  id="métodos-de-regularización">

<ul>
<li><p>Alternativa a mínimos cuadrados con selección de regresores</p></li>
<li><p>Ajustar un modelo que contenga <strong>todos</strong> los regresores, PERO con una técnica que limite las estimaciones de los coeficientes, o las reduzca a cero.</p></li>
<li><p>A priori, NO es obvio por qué esa restricción debería mejorar el ajuste, pero esto reduce su varianza</p></li>
<li><p>Dos enfoques</p>

<ul>
<li>&ldquo;Ridge regression&rdquo;: se reducen los coeficientes</li>
<li>LASSO (&ldquo;least absolute shrinkage and selection operator&rdquo;): selección automática de regresores</li>
</ul></li>
<li><p>Regresión de red elástica, incorpora ambos</p></li>
</ul>

</article></slide><slide class=""><hgroup><h2>&ldquo;Ridge Regression&rdquo;</h2></hgroup><article  id="ridge-regression">

<ul>
<li><p>En <em>MCO</em>: \(\small \min_{\beta}=SCR={\sum_{i=1}^{n}\left(y-\widehat{y}\right)^2}\)</p></li>
<li><p>Añadir restricciones para prevenir &ldquo;overfitting&rdquo; \(\small \sum_{j=1}^{p}\beta_j^2 \leq c\)</p></li>
<li><p>Se obtiene un coeficiente estimado \(\small \widehat{\beta}^R_{\lambda}\) que minimiza \[
SCR + \lambda \sum_{j=1}^{p}\beta_j^2 = SCR + \lambda ||\beta||_2^2
\]</p>

<ul>
<li><p>\(||\beta||_2 = \sqrt{\sum_{j=1}^{p}\beta_j^2}\) es la norma L2 (\(\ell_2\)) del vector de coeficientes</p></li>
<li><p>\(\lambda \geq 0\) es un parámetro de ajuste (&ldquo;tuning parameter&rdquo;)</p></li>
</ul></li>
</ul>

</article></slide><slide class=""><hgroup><h2>&ldquo;Ridge Regression&rdquo;: penalización de contracción</h2></hgroup><article  id="ridge-regression-penalización-de-contracción">

<p>\[\small
\widehat{\beta}^R_{\lambda} = \arg \min_\beta SCR + \lambda \sum_{j=1}^{p}\beta_j^2 
\]</p>

<ul>
<li><p>Tratamos de ajustarnos a los datos minimizando SCR, PERO se recompensa a los coeficientes cercanos a cero: <strong>penalización de contracción</strong></p></li>
<li><p>NO se penaliza a la constante (media de \(\small Y\)), solo el impacto de \(\small X\)</p></li>
<li><p>\(\small \lambda\)= importancia de la penalización (cuanto se contraen los coeficientes)</p>

<ul>
<li><p>\(\small \lambda \approx 0\), cercano a MCO</p></li>
<li><p>\(\small \lambda &gt;&gt; 0\), todos los coeficientes se van a cero</p></li>
</ul></li>
<li><p>Ventaja sobre la selección de regresores: SOLO necesitamos ajustar un modelo para cada valor de \(\small \lambda\)</p></li>
</ul>

</article></slide><slide class=""><hgroup><h2>&ldquo;Ridge Regression&rdquo;: Advertencia</h2></hgroup><article  id="ridge-regression-advertencia">

<ul>
<li><p>En MCO los coeficientes estimados son equivariantes a la escala de los regresores. En &ldquo;Ridge Regression&rdquo;, NO.</p></li>
<li><p>En MCO, si \(\small X_j\) por una constante, \(\small c\), el coeficiente estimado se reescala por \(\small 1/c\) y el valor predicho \(\widehat{\beta_j}X_j\) sigue siendo el mismo.</p></li>
<li><p>\(SCR\) no cambia cuando se reescala un regresor, PERO la penalización SÍ</p></li>
<li><p>Los coeficientes estimados de &ldquo;ridge regression&rdquo; pueden cambiar drásticamente después de reescalar cualquier variable</p></li>
<li><p>Se recomienda ajustar &ldquo;ridge regression&rdquo; <strong>después de estandarizar</strong> los regresores: \[
\small
\widetilde{x}_{ij} = \frac{x_{ij}}{\sqrt{ \frac{1}{n}\sum_{i=1}^n(x_{ij}-\bar{x}_j)^2}}
\]</p></li>
</ul>

</article></slide><slide class=""><hgroup><h2>&ldquo;Ridge Regression&rdquo;: &ldquo;trade-off&rdquo; sesgo varianza</h2></hgroup><article  id="ridge-regression-trade-off-sesgo-varianza">

<ul>
<li><p>¿Por qué &ldquo;ridge regression&rdquo; mejoraría el ajuste sobre MCO? Por el &ldquo;trade-off&rdquo; entre sesgo y varianza</p>

<ul>
<li><p>\(\small \lambda\) aumenta (menos parecido a MCO), la flexibilidad disminuye: más sesgo, menos varianza</p></li>
<li><p>\(\small \lambda\) disminuye, la flexibilidad aumenta: menos sesgo, más varianza</p></li>
</ul></li>
<li><p>&ldquo;Ridge regression&rdquo; funciona mejor cuando MCO tiene alta varianza: intercambia un poco más de sesgo por una gran reducción de la varianza</p></li>
<li><p>Sin embargo, NO realiza selección de variables: sigue incluyendo <em>todos</em> los regresores</p>

<ul>
<li>puede complicar la interpretación con muchos, porque ninguno será exactamente cero</li>
</ul></li>
</ul>

</article></slide><slide class=""><hgroup><h2>LASSO</h2></hgroup><article  id="lasso">

<ul>
<li>Idea similar \(\small \min_{\beta}=SCR={\sum_{i=1}^{n}\left(y-\widehat{y}\right)^2}\), sujeto a \(\small \sum_{j=1}^{p}\beta_j^2 \leq c\)</li>
</ul>

<!--
* Selección de mejor conjunto impone restricción
$\small \sum_{j=1}^{p} I(\beta_j \neq 0) \leq s$

Tampoco es factible: requiere considerar todos los modelso que tiene s regresores

LASSO/ridge  más factibles computacionalmente: sustituyen unas restricciones intratablse por alternativas mucho más fáciles de resolver
-->

<p>\[
\widehat{\beta}^L_{\lambda} = \arg \min_\beta SCR + \lambda \sum_{j=1}^{p}|\beta_j| 
\]</p>

<ul>
<li><p>LASSO utiliza una penalización basada en la norma L1 (\(\small \ell_1\)): \[\small
||\beta||_1 = \sum_{j=1}^{p} |\beta_j|
\]</p></li>
<li><p>También contrae los coeficientes estimados hacia cero, PERO obliga algunos a ser <strong>exactamente iguales a cero</strong> cuando \(\small \lambda\) es grande</p></li>
<li><p>LASSO realiza la selección de variables.</p></li>
</ul>

</article></slide><slide class=""><hgroup><h2>LASSO</h2></hgroup><article  id="lasso-1">

<center>

<img width="90%" src='pic/lasso.png' title=''/>

</center>

</article></slide><slide class=""><hgroup><h2>LASSO vs. &ldquo;Ridge Regression&rdquo;</h2></hgroup><article  id="lasso-vs.-ridge-regression">

<ul>
<li><p>Ambos reducen significativamente la varianza a expensas de un pequeño aumento del sesgo</p></li>
<li><p>&ldquo;Ridge regression&rdquo; domina cuando hay muchos regresores igualmente importantes</p></li>
<li><p>LASSO domina cuando hay un pequeño número de regresores importantes y muchos otros que no son útiles</p></li>
<li><p>Generalización: Regresión de red elástica</p></li>
</ul>

<p>\[\small
\min_{\beta}=SCR+\lambda \big[ (1-\alpha)||\beta||_2^2/2 + \alpha||\beta||_1\big]
\]</p>

</article></slide><slide class=""><hgroup><h2>Eligiendo el parámetro de ajuste</h2></hgroup><article  id="eligiendo-el-parámetro-de-ajuste">

<ul>
<li><p>Necesitamos un método para determinar qué modelo es el mejor: validación cruzada.</p></li>
<li><p>Un algoritmo general para seleccionar un parámetro de ajuste:</p>

<ol>
<li><p>Elegir un rango de valores para \(\small \lambda\)</p></li>
<li><p>Calcular el error mediante validación cruzada para cada valor de \(\small \lambda\)</p></li>
<li><p>Seleccionar el valor con menor error</p></li>
<li><p>Volver a ajustar el modelo usando todas las observaciones y el valor del parámetro de ajuste seleccionado.</p></li>
</ol></li>
</ul>

</article></slide><slide class=""><hgroup><h2><code>glmnet</code> para regresión lineal</h2></hgroup><article  id="glmnet-para-regresión-lineal">

<pre class = 'prettyprint lang-r'>library(mosaicData)
library(glmnet)

x &lt;- model.matrix(volume ~ spring + summer + fall + weekday + poly(avgtemp, 6), data = RailTrail)

fit.lmreg &lt;- glmnet(x = x, y = RailTrail$volume, family=&quot;gaussian&quot;,lambda=0.5, alpha=0.5)
coef(fit.lmreg)</pre>

<ul>
<li>Elegimos el parámetero de regularización mediante validación cruzada</li>
</ul>

<pre class = 'prettyprint lang-r'>set.seed(1)
cv.glmnet(x, RailTrail$volume) %&gt;% plot()</pre>

</article></slide><slide class=""><hgroup><h2><code>glmnet</code> para regresión logística</h2></hgroup><article  id="glmnet-para-regresión-logística">

<pre class = 'prettyprint lang-r'>censo &lt;- read_csv(&quot;https://www.dropbox.com/s/6bqyjnkd2c638rm/census.csv?dl=1&quot;) %&gt;%
  mutate(income = as.integer(factor(income))-1)

x &lt;- model.matrix(income ~ education + relationship + poly(age,2) + 
                    workclass + occupation, 
              family = &quot;binomial&quot;, data = censo)

fit.glmreg &lt;- glmnet(x = x, y = censo$income, lambda=0.001, alpha=1)
coef(fit.glmreg)

set.seed(1)  # validación cruzada para elegir parámetro de regularización
cv.glmnet(x, censo$income) %&gt;% plot()</pre></article></slide>


  <slide class="backdrop"></slide>

</slides>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!-- map slide visiblity events into shiny -->
<script>
  (function() {
    if (window.jQuery) {
       window.jQuery(document).on('slideleave', function(e) {
         window.jQuery(e.target).trigger('hidden');
      });
       window.jQuery(document).on('slideenter', function(e) {
         window.jQuery(e.target).trigger('shown');
      });
    }
  })();
</script>

</body>
</html>
