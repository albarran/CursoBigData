<!DOCTYPE html>
<html>
<head>
  <title>Tema 08 - Aprendizaje Estadístico</title>

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="generator" content="pandoc" />




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">

  <base target="_blank">

  <script type="text/javascript">
    var SLIDE_CONFIG = {
      // Slide settings
      settings: {
                title: 'Tema 08 - Aprendizaje Estadístico',
                        useBuilds: true,
        usePrettify: true,
        enableSlideAreas: true,
        enableTouch: true,
                        favIcon: 'Tema08sl_files/logo.svg',
              },

      // Author information
      presenters: [
            {
        name:  'Prof.: Pedro Albarrán' ,
        company: '',
        gplus: '',
        twitter: '',
        www: '',
        github: ''
      },
            {
        name:  'Prof.: Alberto Pérez' ,
        company: '',
        gplus: '',
        twitter: '',
        www: '',
        github: ''
      },
            ]
    };
  </script>

  <link href="site_libs/ioslides-13.5.1/fonts/fonts.css" rel="stylesheet" />
  <link href="site_libs/ioslides-13.5.1/theme/css/default.css" rel="stylesheet" />
  <link href="site_libs/ioslides-13.5.1/theme/css/phone.css" rel="stylesheet" />
  <script src="site_libs/ioslides-13.5.1/js/modernizr.custom.45394.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/prettify/prettify.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/prettify/lang-r.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/prettify/lang-yaml.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/hammer.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/slide-controller.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/slide-deck.js"></script>

  <style type="text/css">

    b, strong {
      font-weight: bold;
    }

    em {
      font-style: italic;
    }

    summary {
      display: list-item;
    }

    slides > slide {
      -webkit-transition: all 0.4s ease-in-out;
      -moz-transition: all 0.4s ease-in-out;
      -o-transition: all 0.4s ease-in-out;
      transition: all 0.4s ease-in-out;
    }

    .auto-fadein {
      -webkit-transition: opacity 0.6s ease-in;
      -webkit-transition-delay: 0.4s;
      -moz-transition: opacity 0.6s ease-in 0.4s;
      -o-transition: opacity 0.6s ease-in 0.4s;
      transition: opacity 0.6s ease-in 0.4s;
      opacity: 0;
    }
/* https://github.com/ropensci/plotly/pull/524#issuecomment-468142578 */
slide:not(.current) .plotly.html-widget{
  display: block;
}

    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
            code.sourceCode > span { display: inline-block; line-height: 1.25; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode { white-space: pre; position: relative; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    code.sourceCode { white-space: pre-wrap; }
    code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
        
    slides > slide:not(.nobackground):before {
      font-size: 12pt;
      content: "";
      position: absolute;
      bottom: 20px;
      left: 60px;
      background: url(Tema08sl_files/logo.svg) no-repeat 0 50%;
      -webkit-background-size: 30px 30px;
      -moz-background-size: 30px 30px;
      -o-background-size: 30px 30px;
      background-size: 30px 30px;
      padding-left: 40px;
      height: 30px;
      line-height: 1.9;
    }
  </style>


</head>

<body style="opacity: 0">

<slides class="layout-widescreen">

  <slide class="title-slide segue nobackground">
        <aside class="gdbar"><img src="Tema08sl_files/logo.svg"></aside>
        <!-- The content of this hgroup is replaced programmatically through the slide_config.json. -->
    <hgroup class="auto-fadein">
      <h1 data-config-title><!-- populated from slide_config.json --></h1>
      
      <p data-config-presenter><!-- populated from slide_config.json --></p>
            <p style="margin-top: 6px; margin-left: -2px;">Universidad de Alicante, Curso 2020/21</p>
          </hgroup>
  </slide>

<slide class=""><hgroup><h2>Aprendizaje Estadístico / Aprendizaje Automático</h2></hgroup><article  id="aprendizaje-estadístico-aprendizaje-automático">

<ul>
<li>Aprendizaje automático (<em>machine learning</em>, ML) o estadístico (<em>statiscal learning</em>): conjunto de técnicas algorítmicas para extraer información de los datos</li>
</ul>

<center>

<img width="65%" src='pic/ML2.jpg' title=''/>

</center>

</article></slide><slide class=""><hgroup><h2>Aprendizaje supervisado vs. no supervisado</h2></hgroup><article  id="aprendizaje-supervisado-vs.-no-supervisado">

<ul>
<li><p><strong>Aprendizaje supervisado</strong>: escenarios en los que para cada observación de las mediciones \(X_i\) hay una respuesta asociada \(Y_i\): \(Y = f(X) + \varepsilon\)</p>

<ul>
<li><p>\(f\) representa la información/relación sistemática que \(X\) (género, educación, etc.) ofrecen sobre un resultado medido \(Y\) (ej. renta)</p></li>
<li><p>\(\varepsilon\) es el error irreducible (variables no observables)</p></li>
<li><p>Aprendemos a predecir el valor o la clase de un individuo a partir de casos previamente &ldquo;etiquetados&rdquo; (medididos/clasificados)</p></li>
</ul></li>
</ul>

<!--
:::: {style="display: flex;"}

::: {}
$\widehat{Y} - Y= \underbrace{\left[\widehat{f}(X) - f(X)\right]}_{(1)} +                  \underbrace{\varepsilon}_{\mbox{(2)}}$
:::

::: {}

$\ \ \ \ \ $

:::

::: {}

1. error reducible (eligiendo modelo)
  
2. error irreducible (variables no observables)

:::

::::
-->

<ul>
<li><p><strong>Aprendizaje no supervisado</strong>: no hay una respuesta asociada a las mediciones de \(X_i\) para supervisar el análisis que generará un modelo.</p>

<ul>
<li>Aprendemos rasgos no medidos a partir de casos &ldquo;no etiquetados&rdquo;: ej. las observaciones de organizan en grupos distintos (de clientes, países)</li>
</ul>

<p><!--
  + *Clustering*: las observaciones se organizan en grupos relativamente distintos (de clientes, de países, etc.)--></p></li>
</ul>

</article></slide><slide class=""><hgroup><h2>Aprendizaje supervisado: estimar \(f\)</h2></hgroup><article  id="aprendizaje-supervisado-estimar-f">

<ul>
<li><p>Modelo paramétrico: supone un forma de \(f\) que depende de parámetros desconocidos, p.e., lineal \(f(x) =\beta_0 + \beta_1 x_1 + \dots + \beta_k x_k\)</p></li>
<li><p>Modelo no paramétrico: estimar \(f\) tan próxima a los datos como sea posible sin supuestos funcionales</p></li>
<li><p>Es más sencillo estimar parámetros que una función arbitraria</p></li>
<li><p>Mejor ajuste cuanto más flexibilidad (método no paramétrico o muchos parámetros), pero puede ajustar &ldquo;demasiado&rdquo; (<em>overfitting</em>)</p></li>
</ul>

</article></slide><slide class=""><hgroup><h2>Aprendizaje supervisado: estimar \(f\) (cont.)</h2></hgroup><article  id="aprendizaje-supervisado-estimar-f-cont.">

<ul>
<li><p>Además existe una disyuntiva entre precisión de la predicción e <strong>interpretabilidad</strong> (inferencia)</p></li>
<li><p>A veces simplemente nos interesa predecir el resultado a partir de unos factores</p></li>
<li><p>Un método más restrictivo puede ser preferible si nos interesa <em>entender</em> la manera en que \(X\) afecta a \(Y\)</p>

<ul>
<li>qué variables son relevantes,</li>
<li>con qué signo y magnitud,</li>
<li>generar hipótesis, etc.</li>
</ul></li>
</ul>

</article></slide><slide class=""><hgroup><h2>Aprendizaje supervisado: problemas de regresión y de clasificación</h2></hgroup><article  id="aprendizaje-supervisado-problemas-de-regresión-y-de-clasificación">

<ol>
<li><p><strong>Regresión</strong>: la variable de respuesta es cuantitativa (toma valores numéricos)</p></li>
<li><p><strong>Clasificación</strong>: la variable de respuesta es cualitativa (toma valores en una de \(K\) categorías o clases)</p></li>
</ol>

<ul>
<li><p>En ambos casos, queremos &ldquo;predecir&rdquo; una variable:</p>

<ul>
<li><p>un valor numérico</p></li>
<li><p>una categoría</p></li>
</ul></li>
<li><p>Clasificación es un caso especial de Predicción.</p></li>
</ul>

<!--
Es importante entender que no trataremos todas las técnicas ni podemos entrar en el fondo de cada técnica.  El objetivo es proporcionar una visión general de alto nivel de las técnicas y modelos empleados habitualmente y así comprender los objetivos generales del aprendizaje automático.
-->

</article></slide><slide class=""><hgroup><h2>Ejemplo de regresión</h2></hgroup><article  id="ejemplo-de-regresión">

<ul>
<li>Predecir el número de usuarios (<code>volume</code>):</li>
</ul>

<pre class = 'prettyprint lang-r'>library(mosaicData)
RailTrail %&gt;% ggplot(aes(x = avgtemp, y = volume)) + 
  geom_point() + geom_smooth(method = &#39;loess&#39;) </pre>

<ul>
<li><p><code>volume</code> &ldquo;supervisa&rdquo; el ajuste del modelo</p></li>
<li><p>Podemos usar el modelo no paramétrico &ldquo;loess&rdquo; para predecir <code>volume</code></p></li>
</ul>

<pre class = 'prettyprint lang-r'>RailTrail.fit &lt;- RailTrail %&gt;% 
  mutate(
    loess.fit = loess(volume ~ avgtemp, data = .)$fitted) 
head(select(RailTrail.fit, volume, loess.fit, avgtemp))</pre>

</article></slide><slide class=""><hgroup><h2>Ejemplo de clasificación</h2></hgroup><article  id="ejemplo-de-clasificación">

<ul>
<li>Clasificación del tipo de flor en los <a href='https://es.wikipedia.org/wiki/Iris_flor_conjunto_de_datos' title=''>datos Iris</a></li>
</ul>

<pre class = 'prettyprint lang-r'>iris %&gt;% mutate(true.Species = Species) %&gt;% 
  ggplot(aes(x = Petal.Length, y = true.Species, color = true.Species)) +   geom_point()</pre>

<ul>
<li><p>Clasificación rudimentaria en función de longitud del pétalo:</p>

<ul>
<li><p>Setosa, si &lt; 2</p></li>
<li><p>Versicolor, si &gt;2 y &lt;5</p></li>
<li><p>Virginica, si &gt; 5</p></li>
</ul></li>
</ul>

<!--
Una vista rápida parece indicar una buena clasificación. La variable de respuesta para la supervisión es `true.Species`.
-->

<ul>
<li><p>La predicción es mucho mejor si se utilizan más variables</p></li>
<li><p>La visualización tiene un poder limitado: debemos usar métodos de clasificación basados en modelos o algoritmos.</p></li>
</ul>

<!--
La predicción es mucho mejor si se utilizan más variables, pero ¿cómo utilizar 4 variables en la clasificación? La visualización en sí misma tiene un poder limitado para contestar esta pregunta. Debemos utilizar métodos de clasificación basados en modelos o algoritmos.

La visualización seguirá siendo útil en el análisis exploratorio (decidir qué método utilizar) y en la transmisión del resultado del análisis.

-->

</article></slide><slide class=""><hgroup><h2>Ejemplo de clasificación (cont.)</h2></hgroup><article  id="ejemplo-de-clasificación-cont.">

<ul>
<li>Factores para predecir si un cliente potencial es de alto ingreso</li>
</ul>

<pre class = 'prettyprint lang-r'>censo &lt;- read_csv(&quot;https://www.dropbox.com/s/6bqyjnkd2c638rm/census.csv?dl=1&quot;) %&gt;%
  mutate(income = as.integer(factor(income))-1)</pre>

<ul>
<li>Ajustamos un modelo logístico (logit) simple</li>
</ul>

<pre class = 'prettyprint lang-r'>modelo_logistico &lt;- glm(income ~ capital_gain, data = censo, family = &quot;binomial&quot;)
summary(modelo_logistico)
cbind(censo$income, predict(modelo_logistico, type = &quot;response&quot;)) %&gt;% head()</pre>

<ul>
<li>La predicción mejora si incluimos más variables explicativas</li>
</ul>

<pre class = 'prettyprint lang-r'>modelo_logistico2 &lt;- glm(income ~ capital_gain + age + education + sex, 
                         data = censo, family = &quot;binomial&quot;)
summary(modelo_logistico2)
cbind(censo$income, predict(modelo_logistico2, type = &quot;response&quot;)) %&gt;% head()</pre>

</article></slide><slide class=""><hgroup><h2>Aprendizaje No Supervisado</h2></hgroup><article  id="aprendizaje-no-supervisado">

<!--
Usamos técnicas en el aprendizaje no supervisado cuando no hay ninguna variable de respuesta. Simplemente tenemos un conjunto de observaciones $X$, y queremos entender las relaciones entre ellos.
-->

<ul>
<li><em>Clustering</em> (agrupamiento o particionamiento): identificar grupos desconocidos de casos a partir de características observadas</li>
</ul>

<!--
Empezamos con ejemplo simple para entender la idea y el funcionamiento de los algoritmos. Usaremos datos de `faithful` (incluido por defecto en R) sobre tiempo de espera entre erupciones y sobre duración de la erupción para el géiser Old Faithful en el Parque Nacional de Yellowstone, EE.UU.

![https://en.wikipedia.org/wiki/Old_Faithful](pic/OldFaithful1948.jpg)
-->

<ul>
<li>Tiempo de espera entre erupciones y sobre duración de la erupción para el géiser Old Faithful</li>
</ul>

<pre class = 'prettyprint lang-r'>faithful%&gt;% ggplot(aes(y = eruptions, x = waiting)) + 
  geom_point()</pre>

<ul>
<li>Se pueden apreciar dos &ldquo;grupos&rdquo; o <em>clusters</em> o tipos de erupciones.</li>
</ul>

<pre class = 'prettyprint lang-r'>faithful.clustered &lt;- 
  faithful %&gt;% mutate(cluster = factor(kmeans(x = ., centers = 2)$cluster))
faithful.clustered %&gt;% ggplot(aes(y = eruptions, x = waiting)) + 
  geom_point(aes(color = cluster))</pre>

<!--
## Evaluación de modelos

* ¿Cómo sabemos que modelo es mejor?

* Dado  $Y = f(X) + \varepsilon$ y una predicción $\widehat{f}$, el promedio de los errores de predicción es:

$$ E\left[\left(y-\widehat{f}(x)\right)^2\right]= \left(E\left[\widehat{f}(x)\right]-f(x)\right)^2 + E\left[\left[\widehat{f}(x)-E\left(\widehat{f}(x)\right)\right]^2\right]+\sigma^2$$


## "Trade-off" Varianza--Sesgo

La disyuntiva entre sesgo y varianza es el problema de minimizar simultáneamente dos fuentes de error que impiden que los algoritmos de aprendizaje supervisados sean generalizables más allá de su conjunto de entrenamiento.

  * El **sesgo** de la estimación es el error por los supuestos erróneos en el algoritmo de aprendizaje. Con un alto sesgo, un algoritmo pierde las relaciones relevantes entre las características y la variable de resultado. En este caso se tiene un ajuste insuficiente (*underfit*).
  
  * La **varianza** de la estimación es un error de sensibilidad a pequeñas fluctuaciones en el conjunto de entrenamiento. Con una alta varianza, un algoritmo modela un puro ruido aleatorio en los datos de entrenamiento. Ahora tendríamos un ajuste excesivo (*overfit*) en la muestra de entrenamiento (pero en la de pruebas gran error de predicción).
  
  El sesgo se reduce y la varianza aumenta en relación con la complejidad del modelo. A medida que se añaden más y más parámetros a un modelo, la complejidad del modelo aumenta y la varianza se convierte en nuestra principal preocupación, mientras que el sesgo disminuye constantemente. Por ejemplo, a medida que se añaden más términos polinómicos a una regresión lineal, mayor será la complejidad del modelo resultante. 


## Medidas de error en predicción

* Criterios habituales para evaluación de modelos de respuesta cuantitativa

+ *Root Mean Square Error*: $RMSE(y,\widehat{y})=\sqrt{\frac{1}{n}\sum_{i=1}^{n}\left(y-\widehat{y}\right)^2}$
     + mismas unidades que $y$
     + penaliza grandes desviaciones
+ *Mean Absolute Error*: $MAE(y,\widehat{y})=\frac{1}{n}\sum_{i=1}^{n}\left|y-\widehat{y}\right|$
     + también mediana

 + *Correlación*: 
      + lineal ($y$ y $\widehat{y}$ pueden no tener las mismas unidades y escala como con RMSE y MAE) 
      + de rangos ($y$ y $\widehat{y}$ solo tiene que tener el mismo orden relativo, no minimizar distancia entre ellas)
 
+ *Coeficiente de determinación*, $R^2$ 
--></article></slide>


  <slide class="backdrop"></slide>

</slides>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!-- map slide visiblity events into shiny -->
<script>
  (function() {
    if (window.jQuery) {
       window.jQuery(document).on('slideleave', function(e) {
         window.jQuery(e.target).trigger('hidden');
      });
       window.jQuery(document).on('slideenter', function(e) {
         window.jQuery(e.target).trigger('shown');
      });
    }
  })();
</script>

</body>
</html>
