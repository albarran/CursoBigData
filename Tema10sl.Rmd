---
#subtitle: "Técnicas para 'Big Data' en Economía"
#subtitle: "Muestreo y Análisis de Datos"
title    :  "Tema 10 - Regresión Logística"
author: 
  - "Prof.: Pedro Albarrán"
  - "Prof.: Alberto Pérez"
job: "Universidad de Alicante" 
date: "Universidad de Alicante, Curso 2020/21"  
output:
#  html_document:
#    toc: true
#     toc_float: true
  ioslides_presentation: 
    widescreen: yes
    logo: pic/by-nc-sa.eu.svg
#  html_document: default
#  beamer_presentation: 
#      slide_level: 2
#widescreen: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, eval = TRUE, echo = TRUE, results = "hide", fig.show="hide")

library(tidyverse)
library(dlookr)
#setwd("/home/albarran/Dropbox/MAD/00.TEC")
library(rmarkdown)
#render("filename.Rmd")     
#browseURL("filename.html")
```


## Regresión Logística

* La regresión lineal puede usarse respuestas binarias (no más de dos categorías),
<!--\[
\small
\Pr(Y=1|X)=\beta_0 + \beta_1 x_1 + \ldots + \beta_k x_k = z
\]
-->
aunque genera predicciones fuera del rango $\small [0,1]$

<!--
* Además, no es adecuado con más de dos categorías
-->

* Solución: aplicar al índice lineal una transformación $\small F(z)\in[0,1]$


:::: {style="display: flex;"}

::: {}

*  La función logística: $\small \Lambda (z)=\frac{e^z}{1+e^z}$
::: 

::: {}
```{r, echo=FALSE, fig.show='asis', fig.height=3}
#install.packages("latex2exp")
library(latex2exp)
Logistic <- function(x) {exp(x)/(1+exp(x))}
ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
    stat_function(fun = Logistic) + scale_x_continuous(limits = c(-5, 5), name = "z") +
  scale_y_continuous(name = TeX("$\\Lambda(z)$")) #+
  annotate("text", x = -3 , y = 0.7, label = TeX("$\\Lambda(z)=\\frac{exp(z)}{1+exp(z)}$", output='character'), parse=TRUE)
```

:::

::::
* De manera que $\small \Pr(Y=1|X)= p(x)= \Lambda( \beta_0 + \beta_1 x_1 + \ldots + \beta_k x_k)$


## Regresión Logística: coeficientes

* Los coeficientes NO se interpretan directamente como cambios en la probabilidad ante cambios unitarios en un regresor

* PERO su signo (y significatividad) son los mismos que los del efecto marginal sobre la probabilidad 

* En esta especificación, la probabilidad relativa ("odd") es 
\[
\small
\frac{p(x)}{1-p(x)}=exp(\beta_0 + \beta_1 x_1 + \ldots + \beta_k x_k)
\]

* Por tanto, su logaritmo ("log odd" o logit) es lineal: los coeficientes son la elasticidad de la probabilidad relativa

## Regresión Logística: estimación 

* Como NO tiene sentido minimizar la SCR, el objetivo es maximizar la probabilidad (verosimilitud) de observar los unos y ceros en los datos
\[
\small
\ell(\beta_0, \beta_1, \dots, \beta_k)=\prod_{i:y_i=1}p(x_i) \prod_{i:y_i=0} \left(1 - p(x_i)\right)
\]

* La regresión logística pertenece a la familia de modelos lineales generalizados (GLM, en inglés) 

* En R, la regresión logística se puede estimar con la función `glm()`

```{r}
library(ISLR)
logit <- glm(data = Default, default ~ balance, family = "binomial" )
```

## Predicciones

* El objeto de R de `glm()` contiene valores predichos, que son probabilidades de $\small Y=1$

* También podemos usar la función `predict()` aplicada al objeto de `glm()` para predecir

    - el índice lineal subyacente
    
    - la probabilidad
    
```{r}
cbind(Default$default, logit$fitted, predict(logit), predict(logit, type="response"))
```

* También se puede predecir usando una muestra distinta de la usada para estimar o con valores concretos de los regresores

```{r}
predict(logit, tibble(balance=c(0,100)), type="response")
```


## Variables explicativas

* Se pueden incluir como variables explicativas tanto variables cuantitativas como cualitativas

```{r}
glm(data = Default, default ~ student, family = "binomial" ) %>% summary()
```


* También transformaciones no lineales de estas e interacciones


* Podemos incurrir en un sesgo por omisión de variables relevantes: ej., el efecto de `student` por omitir `balances` (con la que está correlacionada)
```{r}
glm(data = Default, default ~ student + balance, family = "binomial" ) %>% summary()
```
## Regresión logística con más de dos clases

* La regresión logística se puede generalizar a situaciones con múltiples clases (modelos multinomiales) con un índice lineal para cada clase
\[
\small
\Pr(Y=c|X)=\frac{e^{\beta_{0c}+\beta_{1c}X_1+\dots+\beta_{kc}X_k}}{\sum_{l=1}^{C}e^{\beta_{0l}+\beta_{1l}X_1+\dots+\beta_{kl}X_k}}
\]

* La librería `glmnet()` permite la estimación de estos modelos

```{r}
library(glmnet)
iris.x <- as.matrix(iris[1:2])
iris.y <- as.matrix(iris[5])
mod.glmnet <- glmnet(x = iris.x, y = iris.y, family = "multinomial", 
                     lambda = 0, type.multinomial = "grouped")
coef(mod.glmnet) 

predict(mod.glmnet, newx=iris.x, type = "response")  # probabilidad de cada clase
predict(mod.glmnet, newx=iris.x, type = "class")     # clase
```

```{r, echo=FALSE, eval=FALSE}
d <- coef(mod.glmnet) %>% reduce(cbind) 
colnames(d) <- names(c)
d
```

```{r, echo=FALSE, eval=FALSE}
library(nnet)
mod.nnet <- multinom(
    Species ~ Sepal.Width + Petal.Length + Petal.Width, # Species ~ .
    data = iris
)
mod.nnet
```

