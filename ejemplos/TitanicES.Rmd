---
  title: "Una familia grande no es buena para sobrevivir (en el Titanic)"
output:
  html_document
---
  
  Una política de "las mujeres y los niños primero" en el Titanic está claramente demostrada por los datos pero parece que tener una familia numerosa tampoco era bueno para las posibilidades de supervivencia.  

Si una persona tenía 5 o más miembros de la familia (incluidos ellos mismos), entonces
es más probable que esa persona no sobreviviera, especialmente en la tercera clase.
que tenía familias enteras a bordo.


# Load data and setup factors
```{r echo=TRUE, message=FALSE, warning=FALSE}
loaddata <- function(file) {
data <- read.csv(file, header = TRUE, stringsAsFactors=F)
# compute family size on dataset (including self)
data$FamilySize <- data$SibSp + data$Parch + 1

data
}

data <- loaddata("../input/train.csv")
# load real test data
titanic_test <- loaddata("../input/test.csv")

# change survived from integer to boolean

data$Survived <- as.logical(data$Survived)
levels(data$Survived) <- c("Not survived", "Survived")

# make explicit factor levels for specific variables: 3=Pclass, 5=Sex, 12=Embarked
for(i in c(3,5,12)) {
data[,i] <- as.factor(data[,i])
}

```

# Age, Sex, and Pclass best predictors for survival

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(ggplot2)
ggplot(data, aes(x=Age, y=Pclass, color=Survived)) + 
geom_jitter(position = position_jitter(height = .1)) +
scale_color_manual(values=c("red", "blue")) + facet_grid(Sex ~ .) +
ggtitle("Age, Sex, and Class as Survival Factors") + ylab("Pclass")
```

##
## Create adjusted family size variable for people sharing cabins but not registered as family members
##

```{r echo=TRUE, message=FALSE, warning=FALSE}
# e.g., if two people assigned to cabin A13 and familysize == 1 then bump up familysize to 2
# combine set of cabins from both test and training data
cabins <- data$Cabin # 1309 rows
n_occur <- data.frame(table(Var1=cabins))	# 187 rows
# remove missing cabin and/or juse the cabin letter code (e.g. D)
n_occur <- subset(n_occur, nchar(as.character(Var1)) > 1) # 183 rows

sharedCabins <- n_occur$Var1[n_occur$Freq > 1]
data$FamilySizeAdj <- data$FamilySize
print(table(data$FamilySize))

sharedInd <- data$FamilySizeAdj == 1 & data$Cabin %in% sharedCabins
data$FamilySizeAdj[sharedInd] <- 2
rowCount <- sum(sharedInd)
print(c("adjusted rows", rowCount)) # 27 rows
print(table(data$FamilySizeAdj))
```

# Break up training set into subset training and test set

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(caret)
set.seed(820)
inTrainingSet <- createDataPartition(data$Survived, p = 0.5, list=FALSE)
train <- data[inTrainingSet,]
test <- data[-inTrainingSet,]
```

# Does adding more variables improve predictions???

```{r echo=TRUE, message=FALSE, warning=FALSE}
modelaccuracy <- function(test, rpred) {
result_1 <- test$Survived == rpred
sum(result_1) / length(rpred)
}

checkaccuracy <- function(accuracy) {
if (accuracy > bestaccuracy) {
bestaccuracy <- accuracy
assign("bestaccuracy", accuracy, envir = .GlobalEnv)
label <- 'better'
} else if (accuracy < bestaccuracy) {
label <- 'worse'
} else {
label <- 'no change'
}
label
}

library(rpart)
# starting with Age and Sex as indicators
fol <- formula(Survived ~ Age + Sex)						# 0.845
rmodel <- rpart(fol, method="class", data=train)
rpred <- predict(rmodel, newdata=test, type="class")
accuracy <- modelaccuracy(test, rpred)
bestaccuracy <- accuracy # init base accuracy
print(c("accuracy1", accuracy))								# baseline
```

# Add Pclass variable

```{r echo=TRUE, message=FALSE, warning=FALSE}
fol <- formula(Survived ~ Age + Sex + Pclass)				# 0.838
rmodel <- rpart(fol, method="class", data=train)
rpred <- predict(rmodel, newdata=test, type="class")
accuracy <- modelaccuracy(test, rpred)
accuracyLabel <- checkaccuracy(accuracy)
# almost as good but little worse
print(c("accuracy2", accuracy, accuracyLabel))				# worse
```

# Substitute Pclass with fare variable

```{r echo=TRUE, message=FALSE, warning=FALSE}
fol <- formula(Survived ~ Age + Sex + Fare)					# 0.807
rmodel <- rpart(fol, method="class", data=train)
rpred <- predict(rmodel, newdata=test, type="class")
accuracy <- modelaccuracy(test, rpred)
accuracyLabel <- checkaccuracy(accuracy)
print(c("accuracy3", accuracy, accuracyLabel))				# worse
```

# Add back Pclass variable

```{r echo=TRUE, message=FALSE, warning=FALSE}
fol <- formula(Survived ~ Age + Sex + Pclass + Fare)		# 0.820
rmodel <- rpart(fol, method="class", data=train)
rpred <- predict(rmodel, newdata=test, type="class")
#print(rmodel)
accuracy <- modelaccuracy(test, rpred)
accuracyLabel <- checkaccuracy(accuracy)
print(c("accuracy4", accuracy, accuracyLabel))				# worse
```
Pclass is better than previous but less so than just age & sex.

# Add SibSp + Parch variables

```{r echo=TRUE, message=FALSE, warning=FALSE}
fol <- formula(Survived ~ Age + Sex + Pclass + Fare + SibSp + Parch) # 0.838
rmodel <- rpart(fol, method="class", data=train)
rpred <- predict(rmodel, newdata=test, type="class")
print(rmodel)
accuracy <- modelaccuracy(test, rpred)
accuracyLabel <- checkaccuracy(accuracy)
print(c("accuracy5", accuracy, accuracyLabel))				# worse
```
SibSp + Parch is little better than previous but still less so than just age & sex.

# Test if deck letter helps predict survivability

```{r echo=TRUE, message=FALSE, warning=FALSE}
# strip off cabin numbers
# Extract the deck number
# first letter : Deck (e.g. A31 -> A)

# make sure Deck in both sets has same levels
# if Test set has T but not Train set different levels causes error in model
train$Deck <- substr(train$Cabin,1,1)
train$Deck[train$Deck==''] = NA
test$Deck <- substr(test$Cabin,1,1)
test$Deck[test$Deck==''] = NA

train$Deck <- as.factor(train$Deck)
test$Deck <- as.factor(test$Deck)

# make Deck have same levels
c <- union(levels(train$Deck), levels(test$Deck))
levels(test$Deck) <- c
levels(train$Deck) <- c

# test if deck letter improves the prediction

fol <- formula(Survived ~ Age + Sex + Pclass + SibSp + Parch + Fare + Deck) # 0.807
rmodel <- rpart(fol, method="class", data=train)
rpred <- predict(rmodel, newdata=test, type="class")
#print(rmodel)
accuracy <- modelaccuracy(test, rpred)
accuracyLabel <- checkaccuracy(accuracy)
print(c("accuracy6", accuracy, accuracyLabel)) 							# 0.807 worse
```

# Check if FamilySize variable is a useful prediction of survivability

```{r echo=TRUE, message=FALSE, warning=FALSE}
fol <- formula(Survived ~ Age + Sex + Pclass + FamilySize)				# 0.872
rmodel <- rpart(fol, method="class", data=train)
rpred <- predict(rmodel, newdata=test, type="class")
print(rmodel)
accuracy <- modelaccuracy(test, rpred)
accuracyLabel <- checkaccuracy(accuracy)
print(c("accuracy7", accuracy, accuracyLabel)) 						# best so far

p <- ggplot(aes(x=Pclass,y=factor(FamilySize),color=Survived),data=data) + 
geom_jitter() + facet_grid(Sex ~ .)
p + ggtitle("Large Family Size >= 5 more likely to not survive") + theme_bw() + 
geom_hline(yintercept=5) + ylab("Family Size")
``` 

**If person had 5 family members or more (including themselves) then more likely the
person didn't survive especially in the third class which had entire families onboard.**
  
  ```{r echo=TRUE, message=FALSE, warning=FALSE}
mosaicplot(table(FamilySize=data$FamilySize, Survived=data$Survived),
           main="Passenger Survival by Family Size",
           color=c("#fb8072", "#8dd3c7"), cex.axis=1.2)
```

**Mosaic plot above clearly shows the drop-off survival at family size of 5 or greater. 
A small family of 2 to 4 people increases chance of survival but 5 or more does not.
Note the size is the number of family members including the passenger so a single
passenger has a family size of 1.**
  
  ```{r echo=TRUE, message=FALSE, warning=FALSE}
# make explicit factor levels for specific variables: Sex + Pclass
titanic_test$Sex <- as.factor(titanic_test$Sex)
titanic_test$Pclass <- as.factor(titanic_test$Pclass)
# now train on entire training set (714 rows)
fol <- formula(Survived ~ Age + Sex + Pclass + FamilySize)
model <- rpart(fol, method="class", data=data)
library(rpart.plot)	
rpart.plot(model,branch=0,branch.type=2,type=1,extra=102,shadow.col="pink",box.col="gray",split.col="magenta",
           main="Decision tree for model")
```

Notice two leaf nodes on the bottom row marked as FALSE with parent logic testing FamilySize variable.  
* Node on left branch Sex=male & Age < 6.5 & FamilySize > 4.    
* Node on right branch Sex=Female & Pclass=3 & FamilySize > 4.  
**Both females and males of large families apparently stayed together and
perished together because all couldn't be saved.**

# Check if removing Pclass changes the overall accuracy

```{r echo=TRUE, message=FALSE, warning=FALSE}
fol <- formula(Survived ~ Sex + Age + FamilySize)				    # 0.854
rmodel <- rpart(fol, method="class", data=train)
rpred <- predict(rmodel, newdata=test, type="class")
accuracy <- modelaccuracy(test, rpred)
accuracyLabel <- checkaccuracy(accuracy)
print(c("accuracy8", accuracy, accuracyLabel)) 	                    # worse
```

## Adjust family size for people sharing cabins but not registered as family members

```{r echo=TRUE, message=FALSE, warning=FALSE}
# does traveling alone contribute to outcome
fol <- formula(Survived ~ Age + Sex + Pclass + FamilySizeAdj)			# 0.872
rmodel <- rpart(fol, method="class", data=train)
rpred <- predict(rmodel, newdata=test, type="class")
# print(rmodel)
accuracy <- modelaccuracy(test, rpred)
accuracyLabel <- checkaccuracy(accuracy)
print(c("accuracy9", accuracy, accuracyLabel)) 							# no change
```

**Too few rows involved so no change**

## Next check if traveling alone boolean variable is better predicter than family size

```{r echo=TRUE, message=FALSE, warning=FALSE}
fol <- formula(Survived ~ Age + Sex + Pclass + TravelAlone)			# 0.843
train$TravelAlone <- train$FamilySize == 1
test$TravelAlone <- test$FamilySize == 1
rmodel <- rpart(fol, method="class", data=train)
rpred <- predict(rmodel, newdata=test, type="class")
accuracy <- modelaccuracy(test, rpred)
accuracyLabel <- checkaccuracy(accuracy)
print(c("accuracy10", accuracy, accuracyLabel)) 			        # worse / no better
```

## Next check if adding Embarked improves the accuracy

```{r echo=TRUE, message=FALSE, warning=FALSE}
fol <- formula(Survived ~ Age + Sex + Pclass + FamilySize + Embarked)	# 0.858 (worse)
rmodel <- rpart(fol, method="class", data=train)
rpred <- predict(rmodel, newdata=test, type="class")
accuracy <- modelaccuracy(test, rpred)
accuracyLabel <- checkaccuracy(accuracy)
print(c("accuracy11", accuracy, accuracyLabel)) 					    # little worse
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
print (c("best accuracy", bestaccuracy))
```

## Summary

**Adding all or many variables as factors to the model is not always a good idea. In fact,
sometimes it makes the prediction accuracy worse as shown above. Better to fine tune the
machine learning process using the right parameters.**

formula								                             | measured accuracy
---------------------------------------------------------------- | -----
1. Survived ~ Age + Sex                        					 | 0.845
2. Survived ~ Age + Sex + Pclass               					 | 0.838
3. Survived ~ Age + Sex + Fare                 					 | 0.807
4. Survived ~ Age + Sex + Pclass + Fare        					 | 0.820
5. Survived ~ Age + Sex + Pclass + Fare + SibSp + Parch 		 | 0.838
6. Survived ~ Age + Sex + Pclass + SibSp + Parch + Fare + Deck   | 0.807
7. Survived ~ Age + Sex + Pclass + FamilySize              		 | 0.872 (*best*)
8. Survived ~ Age + Sex + FamilySize                       	     | 0.854
9. Survived ~ Age + Sex + Pclass + FamilySizeAdj                 | 0.872 (no change)
10. Survived ~ Age + Sex + Pclass + TravelAlone             	 | 0.843
11. Survived ~ Age + Sex + Pclass + FamilySize + Embarked   	 | 0.858

**The best prediction for survival from the analysis above was based on the formula(Survived ~ Age + Sex + Pclass + FamilySize)
as applied to Recursive Partitioning and Regression Trees (aka rpart).**

Changing the starting random seed and/or using other training methods (e.g. randomForest, cforest, etc.)
may have different results. FamilySize is also a good predictor to include in cforest-based machine learning.